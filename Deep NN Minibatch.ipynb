{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, nn_architecure):\n",
    "        self.nn_architecture = nn_architecture\n",
    "        self.nb_layers = len(nn_architecture)\n",
    "        \n",
    "        #Initialises the network with random weights and zero biases \n",
    "        #creates a parameters dictionary with W1, W2... and b1, b2... the numpy arrays/matrices\n",
    "        self.parameters = {}\n",
    "        for layer in range(1, self.nb_layers):\n",
    "            self.parameters['W'+str(layer)] = numpy.random.randn(self.nn_architecture[layer][\"layer_size\"], self.nn_architecture[layer - 1][\"layer_size\"])\n",
    "            \n",
    "            self.parameters['b'+str(layer)] = numpy.zeros((self.nn_architecture[layer][\"layer_size\"],1))\n",
    "        \n",
    "        pass\n",
    "\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        forward_memory = {'A0':inputs}\n",
    "        current_A = inputs\n",
    "        for layer in range(1, self.nb_layers):\n",
    "            W, b = self.parameters['W'+str(layer)], self.parameters['b'+str(layer)]\n",
    "            activation = self.nn_architecture[layer][\"activation\"]\n",
    "            \n",
    "            Z, new_A = linear_activation(W, b, current_A, activation)\n",
    "            \n",
    "            #keep every activation in a memory dictionary\n",
    "            forward_memory['Z'+str(layer)] = Z\n",
    "            forward_memory['A'+str(layer)] = new_A\n",
    "            \n",
    "            current_A = new_A\n",
    "            \n",
    "        return new_A, forward_memory\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    def backward_pass(self, output, target, forward_memory):\n",
    "        #First find the first error with MSE derivative\n",
    "        grads = {}\n",
    "        target = numpy.reshape(target,(10,1))\n",
    "        prev_dA = output - target\n",
    "        \n",
    "        #computes the derivatives for each layer\n",
    "        for layer in reversed(range(1,self.nb_layers)):\n",
    "            #fetch relevant parameters\n",
    "            W, b = self.parameters['W'+str(layer)], self.parameters['b'+str(layer)]\n",
    "            activation = self.nn_architecture[layer][\"activation\"]\n",
    "            \n",
    "            A, Z = forward_memory['A'+str(layer-1)],forward_memory['Z'+str(layer)]\n",
    "            \n",
    "            #Find derivatives\n",
    "            dA, dW, db = linear_activation_backwards(A, Z, W, activation, prev_dA)\n",
    "            \n",
    "            #grads is a dictionary storing the gradients\n",
    "            grads['dW'+str(layer)] = dW\n",
    "            grads['db'+str(layer)] = db\n",
    "            \n",
    "            prev_dA = dA\n",
    "            \n",
    "        return grads\n",
    "            \n",
    "        \n",
    "    def update_params(self, grads, learning_rate):  #gradient descent's incremental downwards step, we update the parameters dictionary\n",
    "        for layer in range(1,self.nb_layers):\n",
    "            self.parameters['W'+str(layer)] = self.parameters['W'+str(layer)] - learning_rate*grads['dW'+str(layer)]\n",
    "            self.parameters['b'+str(layer)] = self.parameters['b'+str(layer)] - learning_rate*grads['db'+str(layer)]\n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions outside of class action\n",
    "\n",
    "#linear_activation, takes the necessary and gives the next layer's activation\n",
    "\n",
    "def linear_activation(W, b, current_A, activation):\n",
    "    Z = linear_forward(W, b, current_A)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        new_A = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        new_A = relu(Z)\n",
    "    \n",
    "    return Z, new_A\n",
    "\n",
    "#linear_forward is just a straightforwards Z = WA + b\n",
    "def linear_forward(W, b, current_A):\n",
    "    Z = numpy.dot(W,current_A) + b\n",
    "    return Z\n",
    "\n",
    "def linear_activation_backwards(A, Z, W, activation, prev_dA):#compute gradients from the params\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = back_sigmoid(Z)*prev_dA\n",
    "    elif activation == \"relu\":\n",
    "        dZ = back_relu(Z)*prev_dA\n",
    "    \n",
    "    #from dZ we can calculate all other derivatives\n",
    "    dA, dW = linear_backwards(A, W, dZ)\n",
    "    db = dZ\n",
    "    \n",
    "    return dA, dW, db\n",
    "    \n",
    "\n",
    "def linear_backwards(A, W, dZ):\n",
    "    dW = numpy.dot(dZ,A.T)\n",
    "    dA = numpy.dot(W.T, dZ)\n",
    "    return dA, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function MSE\n",
    "def MSE(yhat, y):\n",
    "    return float((1/2)*numpy.dot((yhat - y).T,(yhat - y)))\n",
    "\n",
    "def MSE_prime(yhat, y):\n",
    "    return (yhat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionary defines the whole network architecture, layer size and activation functions are the two parameters\n",
    "nn_architecture = [\n",
    "    {\"layer_size\":784, \"activation\": \"none\"},\n",
    "    {\"layer_size\": 300, \"activation\":\"sigmoid\"},\n",
    "    {\"layer_size\": 100, \"activation\":\"relu\"},\n",
    "    {\"layer_size\": 10, \"activation\":\"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+numpy.exp(-x))\n",
    "\n",
    "def back_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "#derivatives\n",
    "def relu(x):\n",
    "    return numpy.maximum(0,x)\n",
    "    \n",
    "def back_relu(x):\n",
    "    x[x>0]= 1\n",
    "    x[x<=0] = 0\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "\n",
    "def create_minibatches(minibatch_size, all_inputs, all_targets):\n",
    "    minibatches = []\n",
    "    data = list(zip(all_inputs,all_targets))\n",
    "    numpy.random.shuffle(data)\n",
    "    \n",
    "    nb_minibatch = len(all_inputs)//minibatch_size\n",
    "    \n",
    "    for batch in range(nb_minibatch+1):\n",
    "        minibatch = data[batch*minibatch_size: (batch+1)*minibatch_size]\n",
    "        X_mini = [minibatch[i][0] for i in range(len(minibatch))]\n",
    "        Y_mini = [minibatch[i][1] for i in range(len(minibatch))]\n",
    "        minibatches.append((X_mini,Y_mini))\n",
    " \n",
    "    \n",
    "    return minibatches\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = open(r\"C:\\Users\\oscar\\Documents\\CS\\ML\\Make your own NN\\mnist_train.csv\")\n",
    "training_data_list = training_data_file.readlines()\n",
    "test_data_file = open(r\"C:\\Users\\oscar\\Documents\\CS\\ML\\Make your own NN\\mnist_test.csv\")\n",
    "test_data_list = test_data_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_targets = []\n",
    "all_test_inputs = []\n",
    "all_test_targets = []\n",
    "for record in range(len(training_data_list)):\n",
    "\n",
    "    # split the record by the ',' commas\n",
    "    all_values = training_data_list[record].split(',')\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    inputs = numpy.reshape(inputs,(784,1))\n",
    "    # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "    targets = numpy.zeros(10) + 0.01\n",
    "    # all_values[0] is the target label for this record\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "    all_inputs.append(inputs)\n",
    "    all_targets.append(targets)\n",
    " #Same thing for test data\n",
    "for record in range(len(test_data_list)):\n",
    "    all_test_values = test_data_list[record].split(',')\n",
    "    inputs = (numpy.asfarray(all_test_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    inputs = numpy.reshape(inputs,(784,1))\n",
    "    targets = numpy.zeros(10) + 0.01\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "    all_test_inputs.append(inputs)\n",
    "    all_test_targets.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent function with minibatches\n",
    "\n",
    "def train(net,inputs,targets,learning_rate, nb_iterations,batch_size):\n",
    "    for iteration in range(nb_iterations):\n",
    "        minibatches = create_minibatches(batch_size,all_inputs,all_targets)\n",
    "        #cost = 0\n",
    "        for minibatch in minibatches:\n",
    "            batch_grads = {}\n",
    "            for example in range(len(minibatch[0])):\n",
    "                output, memo = net.forward_pass(minibatch[0][example])\n",
    "                #t = numpy.reshape(minibatch[1][example],(10,1))\n",
    "                #cost += float(MSE(output, t))\n",
    "                grads = net.backward_pass(output, minibatch[1][example], memo)\n",
    "                \n",
    "                for layer in range(1,net.nb_layers):\n",
    "                    if (example == 0):  #initialising dictionary on the first example\n",
    "                        batch_grads['dW'+str(layer)] = grads['dW'+str(layer)]\n",
    "                        batch_grads['db'+str(layer)] = grads['db'+str(layer)]\n",
    "                    else:\n",
    "                        batch_grads['dW'+str(layer)] = grads['dW'+str(layer)] + batch_grads['dW'+str(layer)]\n",
    "                        batch_grads['db'+str(layer)] = grads['db'+str(layer)] + batch_grads['db'+str(layer)]\n",
    "        \n",
    "            \n",
    "            net.update_params(batch_grads,learning_rate/batch_size)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(nn_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, all_inputs, all_targets, learning_rate = 0.01, nb_iterations = 2, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate network accuracy on training data\n",
    "c = 0\n",
    "for example in range(len(all_inputs)):\n",
    "    output, memo = net.forward_pass(all_inputs[example])\n",
    "    guess, truth = numpy.argmax(output), numpy.argmax(all_targets[example])\n",
    "    if guess == truth:\n",
    "        c +=1\n",
    "print('accuracy', c/len(all_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate network accuracy\n",
    "c = 0\n",
    "for example in range(len(all_test_inputs)):\n",
    "    output, memo = net.forward_pass(all_test_inputs[example])\n",
    "    guess, truth = numpy.argmax(output), numpy.argmax(all_test_targets[example])\n",
    "    if guess == truth:\n",
    "        c +=1\n",
    "print('accuracy', c/(len(all_test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, memo = net.forward_pass(all_inputs[0])\n",
    "        \n",
    "            \n",
    "grads =net.backward_pass(output, all_targets[0], memo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
